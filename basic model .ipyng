!pip install transformers torch langchain langgraph sentence-transformers faiss-cpu fastapi

# Import required libraries
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.agents import initialize_agent, Tool
from langchain.agents import AgentType

# Load GPT-2 pre-trained model and tokenizer
model_name = "gpt2"
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Ensure model is in eval mode
model.eval()


from transformers import Trainer, TrainingArguments

# Prepare your dataset and dataloaders here
train_dataset = ...
eval_dataset = ...

training_args = TrainingArguments(
    output_dir='./results', num_train_epochs=3, per_device_train_batch_size=4,
    per_device_eval_batch_size=8, logging_dir='./logs',
)

trainer = Trainer(
    model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset,
)

trainer.train()

# Initialize the SentenceTransformer model for embedding generation
embed_model = SentenceTransformer('all-MiniLM-L6-v2')

# Example sentences to generate embeddings
texts = ["What is AI?", "Explain GPT-2 model.", "Langchain can chain models together."]
embeddings = embed_model.encode(texts)

# Display the embeddings
print(embeddings)


# Initialize FAISS index (L2 distance)
dimension = embeddings.shape[1]  # Embedding dimension
index = faiss.IndexFlatL2(dimension)

# Convert embeddings to numpy array and add them to FAISS index
embeddings_np = np.array(embeddings).astype('float32')
index.add(embeddings_np)

# Example query to search for similar items
query = "What is artificial intelligence?"
query_embedding = embed_model.encode([query]).astype('float32')

# Search for the top 2 nearest neighbors
k = 2
distances, indices = index.search(query_embedding, k)

print(f"Nearest neighbors for '{query}':")
for idx, distance in zip(indices[0], distances[0]):
    print(f"Text: {texts[idx]}, Distance: {distance}")

# Define function to query FAISS index
def query_faiss(query):
    query_embedding = embed_model.encode([query]).astype('float32')
    distances, indices = index.search(query_embedding, k=2)
    results = [texts[idx] for idx in indices[0]]
    return results

# Define function to query GPT-2 for responses
def gpt2_query(context, question):
    input_text = f"Context: {context}\nQuestion: {question}"
    inputs = tokenizer(input_text, return_tensors="pt")
    outputs = model.generate(inputs['input_ids'], max_length=150, num_return_sequences=1)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Setup Langchain tools
tools = [
    Tool(name="FAISS Query", func=query_faiss, description="Query the FAISS index for relevant information"),
    Tool(name="GPT-2 Model", func=gpt2_query, description="Generate a response with GPT-2 model")
]

# Initialize Langchain agent with the tools
llm = OpenAI(model_name="gpt-2", temperature=0)
agent = initialize_agent(tools, llm, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

def ask_question(question):
    response = agent.run({"input": question})
    return response

# Example question
question = "What is GPT-2?"
answer = ask_question(question)
print(answer)
